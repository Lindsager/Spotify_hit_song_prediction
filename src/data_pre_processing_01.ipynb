{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e492deff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# read-in data sources and pre-process by filtering columns, removing punctuation/capitalization, date formatting\n",
    "# normalize song/artist fields, creating universal song_artist field for merging\n",
    "def pre_process_billboard_source(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['year'] = pd.to_datetime(data['Date'], errors='coerce').dt.year\n",
    "    data = data[['Song', 'Artist', 'year']]\n",
    "    data.rename(columns={'Song': 'song', 'Artist': 'artist'}, inplace= True)\n",
    "    data['song'] = data['song'].str.lower().str.strip()\n",
    "    data['artist'] = data['artist'].str.lower().str.strip()\n",
    "    data['song_artist'] = data['song'] + \"_\" + data['artist']\n",
    "    \n",
    "    return data.drop_duplicates(subset=['song','artist'])\n",
    "    \n",
    "\n",
    "def pre_process_billboard_with_features(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data[['Performer', 'Song', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
    "                 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']]\n",
    "    data = data.dropna()\n",
    "    data.rename(columns={'Song':'song', 'Performer':'artist'}, inplace=True)\n",
    "    data['song'] = data['song'].str.lower().str.strip()\n",
    "    data['artist'] = data['artist'].str.lower().str.strip()\n",
    "    data['song_artist'] = data['song'] + \"_\" + data['artist']\n",
    "    \n",
    "    return data.drop_duplicates(subset=['song','artist'])\n",
    "\n",
    "def pre_process_spotify_mil_2000_2023(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.rename(columns={'track_name': 'song', 'artist_name': 'artist'}, inplace=True)\n",
    "    df['song'] = df['song'].str.lower().str.strip()\n",
    "    df['artist'] = df['artist'].str.lower().str.strip()\n",
    "    df['song_artist'] = df['song'] + \"_\" + df['artist']\n",
    "    \n",
    "    return df.drop_duplicates(subset=['song','artist'])\n",
    "\n",
    "\n",
    "def pre_processs_spotify_1mil(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    correction_year = (df['year'] == 0) & (df['album'] == 'Optimism 2') & (df['artists'].str.contains('icizzle'))\n",
    "    df.loc[correction_year, 'year'] = 2018\n",
    "    df.rename(columns={'name': 'song', 'artists': 'artist'}, inplace=True)\n",
    "    df['artist'] = df['artist'].str.replace(r\"[\\[\\]()']\", '', regex=True)\n",
    "    df['song'] = df['song'].str.lower().str.strip()\n",
    "    df['artist'] = df['artist'].str.lower().str.strip()\n",
    "    df['song_artist'] = df['song'] + \"_\" + df['artist']\n",
    "    \n",
    "    return df.drop_duplicates(subset=['song','artist'])\n",
    "\n",
    "# merge the non-hit song datasets and drop duplicate songs\n",
    "def merge_1mil_datasets(df1, df2):\n",
    "    conserved_columns = list(df1.columns.intersection(df2.columns))\n",
    "    merged_df = pd.merge(df1[conserved_columns], df2[conserved_columns], on=conserved_columns, how='outer')\n",
    "    merged_df_unique = merged_df.drop_duplicates(subset=['song_artist'], keep='first')\n",
    "    \n",
    "    return merged_df_unique\n",
    "\n",
    "\n",
    "#merge billboard top songs datasets, assign audio features to our source of truth from the other, less complet dataset\n",
    "def merge_hit_song_datasets(data_hits, billboard_hits_with_features):\n",
    "    common_columns_hits = ['song', 'artist', 'song_artist']\n",
    "    top_hits_all = pd.merge(data_hits, billboard_hits_with_features, on=common_columns_hits, how='left')\n",
    "    return top_hits_all\n",
    "\n",
    "\n",
    "# for any hit songs that don't have acssociated audio feature values, see if 2 mil dataset contains those songs\n",
    "# merge hit song data from 2mil song dataset into hit songs dataset and then remove hit song data from 2mil dataset all together\n",
    "def create_and_label_hit_and_nonhit_datasets(df_2mil, billboard_hits):\n",
    "    overlap_tracks = df_2mil[df_2mil['song_artist'].isin(billboard_hits['song_artist'])]\n",
    "    common_columns = list(billboard_hits.columns.intersection(overlap_tracks.columns))\n",
    "    \n",
    "    merged_hits_features = pd.merge(overlap_tracks, billboard_hits, on=common_columns, how='outer')\n",
    "    merged_hits_features = merged_hits_features.drop_duplicates(subset='song_artist', keep='first')\n",
    "    merged_hits_features = merged_hits_features[[col for col in merged_hits_features.columns if not col.endswith('_y')]]\n",
    "    merged_hits_features = merged_hits_features.dropna()\n",
    "    \n",
    "    #label new column of resulting dataframes with 1=hit song and 0=non-hit song, this will be our prediction column\n",
    "    merged_hits_features.loc[:,'hit_song'] = 1\n",
    "    \n",
    "    # create non-hit songs dataset by excluding overlaps from the hit songs dataset\n",
    "    non_hit_songs = df_2mil[~df_2mil['song_artist'].isin(billboard_hits['song_artist'])]\n",
    "    non_hit_songs.loc[:,'hit_song'] = 0\n",
    "    \n",
    "    return merged_hits_features, non_hit_songs\n",
    "\n",
    "#save the intermediate outputs for use in the next portion of the pipeline\n",
    "def save_processed_data(df, filepath):\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_hits = pre_process_billboard_source('../data/raw/charts_billboard_1958_2024.csv')\n",
    "    df_hits = pre_process_billboard_with_features('../data/raw/hot_100_with_audio_features.csv')\n",
    "    spotify_mil_2000_2023 = pre_process_spotify_mil_2000_2023('../data/raw/spotify_data.csv')\n",
    "    spotify_1mil = pre_processs_spotify_1mil('../data/raw/spotify_1million.csv')\n",
    "\n",
    "    top_hits_all = merge_hit_song_datasets(data_hits, df_hits)\n",
    "\n",
    "    merged_spotify_datasets = merge_1mil_datasets(spotify_1mil, spotify_mil_2000_2023)\n",
    "\n",
    "    hits_features, non_hits = create_and_label_hit_and_nonhit_datasets(merged_spotify_datasets, top_hits_all)\n",
    "\n",
    "    save_processed_data(hits_features, '../data/processed/top_hits_features.csv')\n",
    "    save_processed_data(non_hits, '../data/processed/non_hit_tracks.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
